{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Semantic Links**"
      ],
      "metadata": {
        "id": "cXNu36WHZWDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "df = pd.read_csv('/content/preprocessed_english_reviews.csv')   #Load data\n",
        "\n",
        "incentive_reviews = df[df['Incentivized'] == 'Incentive']['preprocessed_CombinedString'].tolist()\n",
        "noincentive_reviews = df[df['Incentivized'] == 'NoIncentive']['preprocessed_CombinedString'].tolist()\n",
        "\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "incentive_embeddings = model.encode(incentive_reviews, show_progress_bar=True)\n",
        "noincentive_embeddings = model.encode(noincentive_reviews, show_progress_bar=True)\n",
        "\n",
        "avg_incentive_embedding = sum(incentive_embeddings) / len(incentive_embeddings)\n",
        "avg_noincentive_embedding = sum(noincentive_embeddings) / len(noincentive_embeddings)\n",
        "\n",
        "similarity = cosine_similarity([avg_incentive_embedding], [avg_noincentive_embedding])\n",
        "print(f\"Cosine Similarity between 'Incentive' and 'NoIncentive' reviews for combined strings: {similarity[0][0]}\")"
      ],
      "metadata": {
        "id": "88bb5MDRSFZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Recommendation and Evaluation**"
      ],
      "metadata": {
        "id": "PUIJUMZ0h3pC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYPLnvJkU_VG"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score  # Ensure these are imported\n",
        "\n",
        "#Load data\n",
        "df = pd.read_csv('', encoding='utf-8')\n",
        "\n",
        "#Split the data\n",
        "main_data, ground_truth_data = train_test_split(df, test_size=0.4, random_state=42)\n",
        "\n",
        "#Filter 'NoIncentive' reviews\n",
        "main_data_no_incentive = main_data[main_data['Incentivized'] == 'NoIncentive']\n",
        "grouped_main_reviews = main_data_no_incentive.groupby('listing_id')['preprocessed_CombinedString'].apply(list)\n",
        "\n",
        "ground_truth_no_incentive = ground_truth_data[ground_truth_data['Incentivized'] == 'NoIncentive']\n",
        "grouped_ground_truth_reviews = ground_truth_no_incentive.groupby('listing_id')['preprocessed_CombinedString'].apply(list)\n",
        "\n",
        "#Initialize the model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "#Embeddings for the main dataset\n",
        "grouped_main_embeddings = {}\n",
        "for listing_id, reviews in grouped_main_reviews.items():\n",
        "    grouped_main_embeddings[listing_id] = model.encode(reviews, show_progress_bar=True)\n",
        "\n",
        "#Embeddings for the ground truth dataset\n",
        "grouped_ground_truth_embeddings = {}\n",
        "for listing_id, reviews in grouped_ground_truth_reviews.items():\n",
        "    grouped_ground_truth_embeddings[listing_id] = model.encode(reviews, show_progress_bar=True)\n",
        "\n",
        "#Preprocess query\n",
        "def preprocess_query(query):\n",
        "    return query.lower()\n",
        "\n",
        "#Embedding of the user query\n",
        "def get_query_embedding(query, model):\n",
        "    preprocessed_query = preprocess_query(query)\n",
        "    return model.encode([preprocessed_query])[0]\n",
        "\n",
        "def find_similar_reviews(query_embedding, grouped_embeddings, top_k=5):\n",
        "    all_results = []\n",
        "    for listing_id, embeddings in grouped_embeddings.items():\n",
        "        similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "        for index in top_indices:\n",
        "            all_results.append((index, similarities[index], listing_id))\n",
        "    sorted_results = sorted(all_results, key=lambda x: x[1], reverse=True)\n",
        "    return sorted_results[:top_k]\n",
        "\n",
        "#Evaluation\n",
        "def evaluate_model_based_on_content(top_reviews, ground_truth_df):\n",
        "    top_review_ids = set([listing_id for _, _, listing_id in top_reviews])\n",
        "    matching_count = sum(ground_truth_df['listing_id'].isin(top_review_ids))\n",
        "    match_ratio = matching_count / len(top_review_ids)\n",
        "    return match_ratio\n",
        "\n",
        "#Match Ratio\n",
        "def calculate_match_ratio(top_reviews, ground_truth_df):\n",
        "    top_review_ids = [listing_id for _, _, listing_id in top_reviews]\n",
        "    matching_count = 0\n",
        "    for review_id in top_review_ids:\n",
        "        if review_id in ground_truth_df['listing_id'].values:\n",
        "            matching_count += 1\n",
        "    return matching_count / len(top_reviews) if top_reviews else 0\n",
        "\n",
        "#Mean Reciprocal Rank (MRR)\n",
        "def calculate_mrr(top_reviews, ground_truth_embeddings):\n",
        "    reciprocal_ranks = []\n",
        "    for _, _, listing_id in top_reviews:\n",
        "        if listing_id in ground_truth_embeddings:\n",
        "            rank = 1\n",
        "            reciprocal_ranks.append(1 / rank)\n",
        "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "\n",
        "\n",
        "def calculate_precision_recall_f1_accuracy(top_reviews, ground_truth_df, total_items):\n",
        "    ground_truth_ids = set(ground_truth_df['listing_id'])\n",
        "    predictions = [listing_id for _, _, listing_id in top_reviews]\n",
        "\n",
        "    # True Positives (TP): Predicted items that are in the ground truth\n",
        "    TP = sum([1 for pred in predictions if pred in ground_truth_ids])\n",
        "\n",
        "    # False Positives (FP): Predicted items not in the ground truth\n",
        "    FP = sum([1 for pred in predictions if pred not in ground_truth_ids])\n",
        "\n",
        "    # False Negatives (FN): Ground truth items not in predictions\n",
        "    FN = sum([1 for true_id in ground_truth_ids if true_id not in predictions])\n",
        "\n",
        "    # Assuming the rest are True Negatives (TN)\n",
        "    TN = total_items - (TP + FP)\n",
        "\n",
        "    # Precision, Recall, F1-Score\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user_query = input(\"Enter your query: \")\n",
        "    query_embedding = get_query_embedding(user_query, model)\n",
        "\n",
        "    top_reviews = find_similar_reviews(query_embedding, grouped_main_embeddings)\n",
        "    match_ratio = calculate_match_ratio(top_reviews, ground_truth_no_incentive)\n",
        "    mrr = calculate_mrr(top_reviews, grouped_ground_truth_embeddings)\n",
        "\n",
        "    print(\"Top 5 Similar Reviews:\")\n",
        "    for review_index, score, listing_id in top_reviews:\n",
        "        print(f\"Listing ID: {listing_id}, Similarity Score: {score:.3f}\")\n",
        "\n",
        "    total_items = len(df)\n",
        "    precision, recall, f1 = calculate_precision_recall_f1(top_reviews, ground_truth_no_incentive)\n",
        "\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall: {recall:.3f}\")\n",
        "    print(f\"F1-Score: {f1:.3f}\")\n",
        "    print(f\"Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"Match Ratio: {match_ratio:.3f}\")\n",
        "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n",
        "\n",
        "    columns = ['Review Index', 'Similarity Score', 'Listing ID']\n",
        "    weights_df = pd.DataFrame(top_reviews, columns=columns)\n",
        "    weights_df.to_csv('/content/listing_id_similarity_scores.csv', index=False)\n"
      ]
    }
  ]
}