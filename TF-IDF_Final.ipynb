{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Semantic Links Between Incentive and NoIncentive Reviews"
      ],
      "metadata": {
        "id": "toP5rNRL6a2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Feature Extraction**"
      ],
      "metadata": {
        "id": "ICWUdjNv6m80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import random\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv()\n",
        "\n",
        "incentive_df = df[df['Incentivized'] == 'Incentive'].sample(n=15000, random_state=1)\n",
        "noincentive_df = df[df['Incentivized'] == 'NoIncentive'].sample(n=15000, random_state=1)\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "incentive_df['processed_review'] = incentive_df['preprocessed_CombinedString'].apply(preprocess)\n",
        "noincentive_df['processed_review'] = noincentive_df['preprocessed_CombinedString'].apply(preprocess)\n",
        "\n",
        "# Extract top n trigrams\n",
        "def get_top_n_trigrams(corpus, n=40):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(3, 3))\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    scores = np.array(X.sum(axis=0)).ravel()\n",
        "    indices = np.argsort(scores)[::-1]\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "    top_features = [(feature_names[i], scores[i]) for i in indices[:n]]\n",
        "    return dict(top_features)\n",
        "\n",
        "top_incentive_trigrams = get_top_n_trigrams(incentive_df['processed_review'])\n",
        "top_noincentive_trigrams = get_top_n_trigrams(noincentive_df['processed_review'])\n",
        "\n",
        "# Graphing\n",
        "def plot_top_trigrams(top_trigrams, title):\n",
        "    trigrams, freqs = zip(*top_trigrams.items())\n",
        "    plt.figure(figsize=(10, 10.5))\n",
        "    bars = plt.barh(trigrams, freqs, color='blue')\n",
        "    plt.barh(trigrams, freqs)\n",
        "    plt.xlabel('TF-IDF Score')\n",
        "    plt.title(f'Top 40 Trigrams in {title} Combined Strings')\n",
        "\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        label_x_pos = width + 0.01\n",
        "        plt.text(label_x_pos, bar.get_y() + bar.get_height() / 2, f'{width:.2f}',\n",
        "                 va='center')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "\n",
        "plot_top_trigrams(top_incentive_trigrams, 'Incentive')\n",
        "plot_top_trigrams(top_noincentive_trigrams, 'NoIncentive')"
      ],
      "metadata": {
        "id": "OZj3MjaV8AHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Evaluate Relationships**"
      ],
      "metadata": {
        "id": "830U-sKRkaxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_trigrams = set(top_incentive_trigrams.keys()) | set(top_noincentive_trigrams.keys())\n",
        "\n",
        "#create vector\n",
        "incentive_vector = [top_incentive_trigrams.get(trigram, 0) for trigram in all_trigrams]\n",
        "noincentive_vector = [top_noincentive_trigrams.get(trigram, 0) for trigram in all_trigrams]\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "incentive_vector_2d = np.array(incentive_vector).reshape(1, -1)\n",
        "noincentive_vector_2d = np.array(noincentive_vector).reshape(1, -1)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity(incentive_vector_2d, noincentive_vector_2d)[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity between Incentive and NoIncentive reviews: {similarity:.3f}\")\n",
        "\n",
        "\n",
        "incentive_scores = list(top_incentive_trigrams.values())\n",
        "noincentive_scores = list(top_noincentive_trigrams.values())\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Perform T-test\n",
        "t_stat, p_value = stats.ttest_ind(incentive_scores, noincentive_scores, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "combined_trigrams = {}\n",
        "for trigram in set(top_incentive_trigrams.keys()).union(set(top_noincentive_trigrams.keys())):\n",
        "    combined_score = top_incentive_trigrams.get(trigram, 0) + top_noincentive_trigrams.get(trigram, 0)\n",
        "    combined_trigrams[trigram] = combined_score\n",
        "\n",
        "#Top 60 trigrams\n",
        "top_60_trigrams = sorted(combined_trigrams.items(), key=lambda x: x[1], reverse=True)[:60]\n",
        "\n",
        "\n",
        "for trigram, score in top_60_trigrams:\n",
        "    print(f\"Trigram: {trigram}, Score: {score}\")\n"
      ],
      "metadata": {
        "id": "TeTKO59rBA-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Recommendation and Evaluation**"
      ],
      "metadata": {
        "id": "dOS8fxdX6zlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/preprocessed_english_reviews.csv', encoding='utf-8')\n",
        "\n",
        "# Split the data\n",
        "main_data, ground_truth_data = train_test_split(df, test_size=0.4, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "main_data_no_incentive = main_data[main_data['Incentivized'] == 'NoIncentive']\n",
        "main_data_no_incentive['preprocessed_reviews'] = main_data_no_incentive['preprocessed_CombinedString'].apply(preprocess_text)\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(3, 3))\n",
        "tfidf_matrix = vectorizer.fit_transform(main_data_no_incentive['preprocessed_reviews'])\n",
        "\n",
        "tfidf_scores_per_listing = {}\n",
        "for idx, listing_id in enumerate(main_data_no_incentive['listing_id']):\n",
        "    if listing_id in tfidf_scores_per_listing:\n",
        "        tfidf_scores_per_listing[listing_id] += tfidf_matrix[idx].toarray()[0]\n",
        "    else:\n",
        "        tfidf_scores_per_listing[listing_id] = tfidf_matrix[idx].toarray()[0]\n",
        "\n",
        "# Normalize the TF-IDF scores\n",
        "for listing_id in tfidf_scores_per_listing:\n",
        "    tfidf_scores_per_listing[listing_id] /= np.linalg.norm(tfidf_scores_per_listing[listing_id])\n",
        "\n",
        "# Preprocess ground_truth_data\n",
        "ground_truth_no_incentive = ground_truth_data[ground_truth_data['Incentivized'] == 'NoIncentive']\n",
        "ground_truth_no_incentive['preprocessed_reviews'] = ground_truth_no_incentive['preprocessed_CombinedString'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "ground_truth_tfidf_matrix = vectorizer.transform(ground_truth_no_incentive['preprocessed_reviews'])\n",
        "\n",
        "#Process query\n",
        "def process_query_and_find_matches(query, vectorizer, tfidf_scores_per_listing):\n",
        "    preprocessed_query = preprocess_text(query)\n",
        "    query_vector = vectorizer.transform([preprocessed_query])\n",
        "\n",
        "    similarities = {listing_id: cosine_similarity(query_vector, np.array([scores])).flatten()[0]\n",
        "                    for listing_id, scores in tfidf_scores_per_listing.items()}\n",
        "\n",
        "    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def calculate_precision_recall_f1_accuracy(top_reviews, ground_truth_df, total_items):\n",
        "    ground_truth_ids = set(ground_truth_df['listing_id'])\n",
        "    predictions = [listing_id for listing_id, _ in top_reviews]\n",
        "\n",
        "    TP = sum([1 for pred in predictions if pred in ground_truth_ids])\n",
        "    FP = sum([1 for pred in predictions if pred not in ground_truth_ids])\n",
        "    FN = sum([1 for true_id in ground_truth_ids if true_id not in predictions])\n",
        "    TN = total_items - (TP + FP)\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "def calculate_match_ratio(top_reviews, ground_truth_df):\n",
        "    top_review_ids = [listing_id for listing_id, _ in top_reviews]\n",
        "\n",
        "    print(\"Top Review IDs:\", top_review_ids)\n",
        "\n",
        "    matching_count = 0\n",
        "    for review_id in top_review_ids:\n",
        "        if review_id in ground_truth_df['listing_id'].values:\n",
        "            matching_count += 1\n",
        "\n",
        "\n",
        "    print(\"Matching Count:\", matching_count)\n",
        "    print(\"Length of Top Reviews:\", len(top_reviews))\n",
        "\n",
        "    # Calculate match ratio\n",
        "    return matching_count / len(top_reviews) if top_reviews else 0\n",
        "\n",
        "def calculate_mrr(top_reviews, ground_truth_df):\n",
        "    reciprocal_ranks = []\n",
        "    ground_truth_ids = set(ground_truth_df['listing_id'])\n",
        "    for rank, (listing_id, _) in enumerate(top_reviews, start=1):\n",
        "        if listing_id in ground_truth_ids:\n",
        "            reciprocal_ranks.append(1 / rank)\n",
        "            break\n",
        "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user_query = input(\"Enter your query: \")\n",
        "    top_matches = process_query_and_find_matches(user_query, vectorizer, tfidf_scores_per_listing)[:5]\n",
        "\n",
        "    print(\"Top 5 Listing IDs:\")\n",
        "    for listing_id, score in top_matches:\n",
        "        print(f\"Listing ID: {listing_id}, Similarity Score: {score:.3f}\")\n",
        "\n",
        "    total_items = len(df)\n",
        "    precision, recall, f1, accuracy = calculate_precision_recall_f1_accuracy(top_matches, ground_truth_data, total_items)\n",
        "    match_ratio = calculate_match_ratio(top_matches, ground_truth_data)\n",
        "    mrr = calculate_mrr(top_matches, ground_truth_data)\n",
        "\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall: {recall:.3f}\")\n",
        "    print(f\"F1-Score: {f1:.3f}\")\n",
        "    print(f\"Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"Match Ratio: {match_ratio:.3f}\")\n",
        "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n",
        "\n",
        "    # Visualization\n",
        "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'Match Ratio', 'MRR']\n",
        "    scores = [precision, recall, f1, accuracy, match_ratio, mrr]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(metrics, scores, marker='o', color='skyblue', linestyle='-', linewidth=2)\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.title('Performance Evaluation')\n",
        "    plt.ylim(-0.1, 1.1)\n",
        "\n",
        "    for i in range(len(scores)):\n",
        "        plt.text(metrics[i], scores[i] + 0.05, f'{scores[i]:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RgcfblKMqCbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}